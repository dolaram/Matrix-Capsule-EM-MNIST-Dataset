{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Capsule Network for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cn1lab005/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import cv2\n",
    "import sys\n",
    "import pong_fun as game # whichever is imported \"as game\" will be used\n",
    "import random\n",
    "import time \n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for the MatrixNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_routing = 2\n",
    "ac_lambda0 = 0.01, \n",
    "#'\\lambda in the activation function a_c, iteration 0')\n",
    "ac_lambda_step = 0.01,\n",
    "#'It is described that \\lambda increases at each iteration with a fixed schedule, however specific super parameters is absent.')\n",
    "epsilon = 1e-9\n",
    "\n",
    "################################\n",
    "A = 32 # , 'number of channels in output from ReLU Conv1')\n",
    "B = 8 # , 'number of capsules in output from PrimaryCaps')\n",
    "C = 16 #, 'number of channels in output from ConvCaps1')\n",
    "D = 16 # , 'number of channels in output from ConvCaps2')\n",
    "is_train = True\n",
    "num_classes = 10\n",
    "train_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = 6 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 500. # timesteps to observe before training\n",
    "EXPLORE = 5000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.05 # final value of epsilon\n",
    "INITIAL_EPSILON = 1.0 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "batch_size = BATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for the MatrixNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_tile(input, kernel, stride):\n",
    "    # output = tf.extract_image_patches(input, ksizes=[1, kernel, kernel, 1], strides=[1, stride, stride, 1], rates=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "    input_shape = input.get_shape()\n",
    "    tile_filter = np.zeros(shape=[kernel, kernel, input_shape[3],\n",
    "                                  kernel * kernel], dtype=np.float32)\n",
    "    for i in range(kernel):\n",
    "        for j in range(kernel):\n",
    "            tile_filter[i, j, :, i * kernel + j] = 1.0\n",
    "\n",
    "    tile_filter_op = tf.constant(tile_filter, dtype=tf.float32)\n",
    "    output = tf.nn.depthwise_conv2d(input, tile_filter_op, strides=[\n",
    "                                    1, stride, stride, 1], padding='VALID')\n",
    "    output_shape = output.get_shape()\n",
    "    output = tf.reshape(output, shape=[-1, int( # -1== int(output_shape[0])\n",
    "        output_shape[1]), int(output_shape[2]), int(input_shape[3]), kernel * kernel])\n",
    "    #print(output.get_shape(),\"fdsggs\")\n",
    "    output = tf.transpose(output, perm=[0, 1, 2, 4, 3])\n",
    "\n",
    "    return output\n",
    "\n",
    "# input should be a tensor with size as [batch_size, caps_num_i, 16]\n",
    "def mat_transform(input, caps_num_c, regularizer, bs):\n",
    "    #batch_size = input.get_shape()[0]\n",
    "    caps_num_i = int(input.get_shape()[1])\n",
    "    output = tf.reshape(input, shape=[-1, caps_num_i, 1, 4, 4])# batch_size = -1\n",
    "    # the output of capsule is miu, the mean of a Gaussian, and activation, the sum of probabilities\n",
    "    # it has no relationship with the absolute values of w and votes\n",
    "    # using weights with bigger stddev helps numerical stability\n",
    "    w = slim.variable('w', shape=[1, caps_num_i, caps_num_c, 4, 4], dtype=tf.float32,\n",
    "                      initializer=tf.truncated_normal_initializer(mean=0.0, stddev=1.0),\n",
    "                      regularizer=regularizer)\n",
    "    #print(\"w\",w.get_shape())\n",
    "    with tf.variable_scope('tile___1'):\n",
    "        w = tf.tile(w, [bs, 1, 1, 1, 1])\n",
    "    #print(\"w\",w.get_shape())\n",
    "    with tf.variable_scope('tile___2'):\n",
    "        output = tf.tile(output, [1, 1, caps_num_c, 1, 1])\n",
    "    with tf.variable_scope('tile___3'):\n",
    "        k = tf.matmul(output, w)\n",
    "        votes = tf.reshape(k, [-1, caps_num_i, caps_num_c, 16]) #batch_size = -1\n",
    "    #votes = tf.reshape(tf.matmul(output, w), [batch_size, caps_num_i, caps_num_c, 16])\n",
    "\n",
    "    return votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_routing(votes, activation, caps_num_c, regularizer, r_in):\n",
    "    test = []\n",
    "\n",
    "    #batch_size = votes.get_shape()[0]\n",
    "    caps_num_i = int(activation.get_shape()[1])\n",
    "    n_channels = int(votes.get_shape()[-1])\n",
    "\n",
    "    sigma_square = []\n",
    "    miu = []\n",
    "    activation_out = []\n",
    "    beta_v = slim.variable('beta_v', shape=[caps_num_c, n_channels], dtype=tf.float32,\n",
    "                           initializer=tf.constant_initializer(0.0),#tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                           regularizer=regularizer)\n",
    "    beta_a = slim.variable('beta_a', shape=[caps_num_c], dtype=tf.float32,\n",
    "                           initializer=tf.constant_initializer(0.0),#tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                           regularizer=regularizer)\n",
    "\n",
    "    # votes_in = tf.stop_gradient(votes, name='stop_gradient_votes')\n",
    "    # activation_in = tf.stop_gradient(activation, name='stop_gradient_activation')\n",
    "    votes_in = votes\n",
    "    #print(votes_in,' = votes')\n",
    "    activation_in = activation\n",
    "\n",
    "    for iters in range(iter_routing):\n",
    "        # if iters == cfg.iter_routing-1:\n",
    "\n",
    "        # e-step\n",
    "        if iters == 0:\n",
    "            r = r_in# tf.constant(np.ones([batch_size, caps_num_i, caps_num_c], dtype=np.float32) / caps_num_c)\n",
    "            #print(r.get_shape(),\"r shape__________\")\n",
    "        else:\n",
    "            # Contributor: Yunzhi Shi\n",
    "            # log and exp here provide higher numerical stability especially for bigger number of iterations\n",
    "            log_p_c_h = -tf.log(tf.sqrt(sigma_square)) - \\\n",
    "                        (tf.square(votes_in - miu) / (2 * sigma_square))\n",
    "            log_p_c_h = log_p_c_h - \\\n",
    "                        (tf.reduce_max(log_p_c_h, axis=[2, 3], keep_dims=True) - tf.log(10.0))\n",
    "            p_c = tf.exp(tf.reduce_sum(log_p_c_h, axis=3))\n",
    "\n",
    "            ap = p_c * tf.reshape(activation_out, shape=[-1, 1, caps_num_c]) # batch_size\n",
    "            #print(ap.get_shape(),\"ap\")\n",
    "            # ap = tf.reshape(activation_out, shape=[batch_size, 1, caps_num_c])\n",
    "\n",
    "            r = ap / (tf.reduce_sum(ap, axis=2, keepdims=True) + epsilon)\n",
    "\n",
    "        # m-step\n",
    "        r = r * activation_in\n",
    "        r = r / (tf.reduce_sum(r, axis=2, keepdims=True)+epsilon)\n",
    "\n",
    "        r_sum = tf.reduce_sum(r, axis=1, keepdims=True)\n",
    "        r1 = tf.reshape(r / (r_sum + epsilon),\n",
    "                        shape=[-1 , caps_num_i, caps_num_c, 1]) # batch_size\n",
    "        #print(r1.get_shape(),\"r1\")\n",
    "        miu = tf.reduce_sum(votes_in * r1, axis=1, keepdims=True)\n",
    "        sigma_square = tf.reduce_sum(tf.square(votes_in - miu) * r1,\n",
    "                                     axis=1, keepdims=True) + epsilon\n",
    "\n",
    "        if iters == iter_routing-1:\n",
    "            r_sum = tf.reshape(r_sum, [-1, caps_num_c, 1])  # batch_size\n",
    "            #print(r_sum.get_shape(),\"r_sum\")\n",
    "            cost_h = (beta_v + tf.log(tf.sqrt(tf.reshape(sigma_square,\n",
    "                                                         shape=[-1, caps_num_c, n_channels])))) * r_sum\n",
    "            #print(cost_h.get_shape(),\"cost_h\") # batch_size\n",
    "            activation_out = tf.nn.softmax(ac_lambda0 * (beta_a - tf.reduce_sum(cost_h, axis=2)))\n",
    "        else:\n",
    "            activation_out = tf.nn.softmax(r_sum)\n",
    "        # if iters <= cfg.iter_routing-1:\n",
    "        #     activation_out = tf.stop_gradient(activation_out, name='stop_gradient_activation')\n",
    "\n",
    "    return miu, activation_out, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coord_add(dataset_name: str):\n",
    "    # TODO: get coord add for cifar10/100 datasets (32x32x3)\n",
    "    options = {'mnist': ([[[8., 8.], [12., 8.], [16., 8.]],\n",
    "                          [[8., 12.], [12., 12.], [16., 12.]],\n",
    "                          [[8., 16.], [12., 16.], [16., 16.]]], 28.),\n",
    "               'smallNORB': ([[[8., 8.], [12., 8.], [16., 8.], [24., 8.]],\n",
    "                              [[8., 12.], [12., 12.], [16., 12.], [24., 12.]],\n",
    "                              [[8., 16.], [12., 16.], [16., 16.], [24., 16.]],\n",
    "                              [[8., 24.], [12., 24.], [16., 24.], [24., 24.]]], 32.)\n",
    "               }\n",
    "    coord_add, scale = options[dataset_name]\n",
    "\n",
    "    coord_add = np.array(coord_add, dtype=np.float32) / scale\n",
    "\n",
    "    return coord_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent for the MatCap DON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork():\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    s = tf.placeholder(tf.float32, shape=(None, 80, 80, 4), name='X')\n",
    "    bs = tf.placeholder(tf.int32, shape=(), name='bs')\n",
    "    #bs = batch_size\n",
    "    r_conv_caps1= tf.placeholder(tf.float32,[None, 72, C], name='r_conv_caps1') # 5*5*batch_size\n",
    "    #r_conv_caps1= tf.placeholder(tf.float32,[5*5*batch_size, 72, C], name='r_conv_caps1') # 5*5*batch_size\n",
    "\n",
    "    r_conv_caps2 = tf.placeholder(tf.float32,[None, 144, D], name='r_conv_caps2') # 3*3*batch_size\n",
    "    #r_conv_caps2 = tf.placeholder(tf.float32,[3*3*batch_size, 144, D], name='r_conv_caps2') # 3*3*batch_size\n",
    "\n",
    "    r_class_caps = tf.placeholder(tf.float32,[None, 16, num_classes], name='r_class_caps') # 3*3*batch_size\n",
    "    #r_class_caps = tf.placeholder(tf.float32,[3*3*batch_size, 16, num_classes], name='r_class_caps') # 3*3*batch_size\n",
    "\n",
    "    #coord_add_op_class_caps  = tf.placeholder(tf.float32,[3*3*batch_size, 16, num_classes, 2], name='coord_add_op_class_caps')\n",
    "                                                                     # 3*3*batch_size\n",
    "    coord_add_op_class_caps  = tf.placeholder(tf.float32,[None, 16, num_classes, 2], name='coord_add_op_class_caps')\n",
    "    # 3*3*batch_size\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    data_size = int(s.get_shape()[1])\n",
    "    # xavier initialization is necessary here to provide higher stability\n",
    "    initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "    # instead of initializing bias with constant 0, \n",
    "    # a truncated normal initializer is exploited here for higher stability \n",
    "    bias_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)  # tf.constant_initializer(0.0)\n",
    "    # The paper didnot mention any regularization, a common l2 regularizer to weights is added here\n",
    "    weights_regularizer = tf.contrib.layers.l2_regularizer(5e-04)\n",
    "    # weights_initializer=initializer,\n",
    "    with slim.arg_scope([slim.conv2d], trainable=is_train, biases_initializer=bias_initializer, weights_regularizer=weights_regularizer):\n",
    "        with tf.variable_scope('relu_conv1'):\n",
    "            output = slim.conv2d(s, num_outputs=A, kernel_size=[10, 10], stride=6, padding='VALID', scope='relu_conv1', activation_fn=tf.nn.relu)\n",
    "            data_size = int(np.floor((data_size - 10) / 6)) + 1\n",
    "            #print(output.get_shape())\n",
    "            #print(data_size)\n",
    "            #assert output.get_shape() == [batch_size, data_size, data_size, 32]\n",
    "            votes__1 = output\n",
    "        with tf.variable_scope('primary_caps'):\n",
    "            pose = slim.conv2d(output, num_outputs=B * 16,kernel_size=[1, 1], stride=1, padding='VALID', scope='primary_caps', activation_fn=None)\n",
    "            activation = slim.conv2d(output, num_outputs=B, kernel_size=[\n",
    "                                     1, 1], stride=1, padding='VALID', scope='primary_caps/activation', activation_fn=tf.nn.sigmoid)\n",
    "            pose = tf.reshape(pose, shape=[-1, data_size, data_size, B, 16]) # (50, 12, 12, 8, 16)\n",
    "\n",
    "            #print(pose.get_shape())\n",
    "            activation = tf.reshape(activation, shape=[-1, data_size, data_size, B, 1]) # (50, 12, 12, 8, 1)\n",
    "            #print(activation.get_shape())\n",
    "            output = tf.concat([pose, activation], axis=4)\n",
    "            output = tf.reshape(output, shape=[-1, data_size, data_size, B * 17]) # (50, 12, 12, 136)\n",
    "            #print(output.get_shape())\n",
    "\n",
    "            #assert output.get_shape() == [batch_size, data_size, data_size, B * 17]\n",
    "        with tf.variable_scope('conv_caps1') as scope:\n",
    "            output = kernel_tile(output, 3, 2)\n",
    "            data_size = int(np.floor((data_size - 2) / 2))\n",
    "            #print(data_size) # 5 \n",
    "            output = tf.reshape(output, shape=[-1, 3 * 3 * B, 17]) \n",
    "            # batch_size * data_size * data_size  (1250, 72, 17) \n",
    "            #print(\"1\",output.get_shape())\n",
    "            activation = tf.reshape(output[:, :, 16], shape=[-1, 3 * 3 * B, 1])\n",
    "            #print(\"output shape ---------------\",output.get_shape())\n",
    "            #print(\"activation shape----------------------\",activation.get_shape()) #  (1250, 72, 1)\n",
    "\n",
    "            with tf.variable_scope('v') as scope:\n",
    "                votes = mat_transform(output[:, :, :16], C, weights_regularizer, bs = bs*data_size*data_size)\n",
    "                #bs*data_size*data_size)\n",
    "\n",
    "                #print(votes.get_shape(),\"votes shape\")\n",
    "            with tf.variable_scope('routing') as scope:\n",
    "                caps_num_i = int(activation.get_shape()[1])\n",
    "\n",
    "                miu, activation, _ = em_routing(votes, activation, C, weights_regularizer,r_conv_caps1)\n",
    "                # miu, activation, _ = em_routing(votes, activation, C, weights_regularizer)\n",
    "                #print(\"activation\",activation.get_shape())\n",
    "            pose = tf.reshape(miu, shape=[-1, data_size, data_size, C, 16])\n",
    "            #print(\"3\",pose.get_shape()) # 50, 5, 5, 16, 16)\n",
    "            activation = tf.reshape(activation, shape=[-1, data_size, data_size, C, 1])\n",
    "            #print(\"activation\",activation.get_shape())\n",
    "            cat_size =  activation.get_shape()[3]*activation.get_shape()[4] + pose.get_shape()[3] *pose.get_shape()[4]\n",
    "            #print(cat_size)\n",
    "            output = tf.reshape(tf.concat([pose, activation], axis=4),[-1, data_size, data_size, cat_size])\n",
    "            #print(\"5\",output.get_shape()) # (50, 5, 5, 272)\n",
    "\n",
    "\n",
    "        with tf.variable_scope('conv_caps2') as scope:\n",
    "            output = kernel_tile(output, 3, 1)\n",
    "\n",
    "            data_size = int(np.floor((data_size - 2) / 1))\n",
    "\n",
    "            output = tf.reshape(output, shape=[-1, 3 * 3 * C, 17]) # batch_size * data_size * data_size\n",
    "            #print(\"canv_caps2\",output.get_shape(), data_size)\n",
    "            activation = tf.reshape(output[:, :, 16], shape=[-1 , 3 * 3 * C, 1]) # batch_size * data_size * data_size\n",
    "            #print(\"canv_caps2_activation\",activation.get_shape(), data_size)\n",
    "\n",
    "            with tf.variable_scope('v') as scope:\n",
    "                votes = mat_transform(output[:, :, :16], D, weights_regularizer,bs = bs*data_size*data_size)\n",
    "                #print(votes.get_shape(),\"votes shape\")\n",
    "\n",
    "            with tf.variable_scope('routing') as scope:\n",
    "                caps_num_i = int(activation.get_shape()[1])\n",
    "                #print(caps_num_i,\"for 1\")\n",
    "                miu, activation, _ = em_routing(votes, activation, D, weights_regularizer, r_conv_caps2)\n",
    "\n",
    "            pose = tf.reshape(miu, shape=[-1, D, 16]) # batch_size * data_size * data_size\n",
    "            #print(\"4\",pose.get_shape())\n",
    "            #tf.logging.info('conv cap 2 pose shape: {}'.format(votes.get_shape()))\n",
    "            activation = tf.reshape(activation, shape=[-1, D, 1]) # batch_size * data_size * data_size\n",
    "            #print(\"4 ---activation\",activation.get_shape())\n",
    "\n",
    "        with tf.variable_scope('class_caps') as scope:\n",
    "            with tf.variable_scope('v') as scope:\n",
    "                votes = mat_transform(pose, num_classes, weights_regularizer,bs = bs*data_size*data_size)\n",
    "                #print(votes.get_shape(),\"votes.getshape\")\n",
    "                #assert votes.get_shape()[1:] == [D, num_classes, 16]\n",
    "                #tf.logging.info('class cap votes original shape: {}'.format(votes.get_shape()))\n",
    "                '''coord_add = get_coord_add('mnist') \n",
    "                coord_add = np.reshape(coord_add, newshape=[data_size * data_size, 1, 1, 2])\n",
    "                coord_add = np.tile(coord_add, [bs, D, num_classes, 1])\n",
    "                coord_add_op = tf.constant(coord_add, dtype=tf.float32)\n",
    "                print(\"___coord_add______\",coord_add_op.shape)'''\n",
    "\n",
    "                votes = tf.concat([coord_add_op_class_caps, votes], axis=3)\n",
    "                #tf.logging.info('class cap votes coord add shape: {}'.format(votes.get_shape()))\n",
    "                #print(votes.get_shape(),\"coorr vote shape after  jnbfv\")\n",
    "            with tf.variable_scope('routing') as scope:\n",
    "                caps_num_i = int(activation.get_shape()[1])\n",
    "                #print(\"_____\",caps_num_i)\n",
    "                miu, activation, test2 = em_routing(votes, activation, num_classes, weights_regularizer,r_class_caps)\n",
    "\n",
    "            output = tf.reshape(activation, shape=[-1, data_size, data_size, num_classes]) #batch_size\n",
    "            #print(\"d op\",output.get_shape())\n",
    "        output = tf.reshape(tf.nn.avg_pool(output, ksize=[1, data_size, data_size, 1], strides=[\n",
    "                    1, 1, 1, 1], padding='VALID'), shape=[-1, num_classes]) # batch_size\n",
    "        #print(\"miu  2\",miu.get_shape())\n",
    "        pose = tf.nn.avg_pool(tf.reshape(miu, shape=[-1, data_size, data_size,miu.get_shape()[2]*miu.get_shape()[3]\n",
    "                                    ]), ksize=[1, data_size, data_size, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "        #print(\"output_size_posssss\",pose.get_shape())\n",
    "        #miu  2 (450, 1, 10, 18)\n",
    "        #output_size_posssss (50, 1, 1, 180)\n",
    "        pose_out = tf.reshape(pose, shape=[-1, num_classes, 18])\n",
    "        #print(\"output_size\",pose_out.get_shape())\n",
    "        vector_j = tf.reshape(pose_out, shape= [-1, num_classes * 18])\n",
    "        #print(\"pose_out\",pose_out.get_shape())\n",
    "        #print(\"vector_j\",vector_j.get_shape())\n",
    "    with tf.variable_scope('output_layer') as scope:\n",
    "        readout = tf.contrib.layers.fully_connected(vector_j, num_outputs=ACTIONS, activation_fn=None) \n",
    "    print(\"Network has been created successfully\")\n",
    "    return s, readout, bs, r_conv_caps1, r_conv_caps2, r_class_caps, coord_add_op_class_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment for Training of the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(sess, s, readout, bs, r_conv_caps1, r_conv_caps2, r_class_caps, coord_add_op_class_caps):\n",
    "    tick = time.time()\n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices = 1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "    # store the previous observations in replay memory\n",
    "    replay_memory = deque()\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal, bar1_score, bar2_score = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2)  \n",
    "    # saving and loading networks\n",
    "    # saver = tf.train.Saver()\n",
    "    # sess.run(tf.initialize_all_variables())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #-------------- initialization for EM routing ------------------------------------\n",
    "    # ----------- for taking actions ---------------------\n",
    "    batch_size_pred = 1\n",
    "    coord_add_p = get_coord_add('mnist') \n",
    "    coord_add_p = np.reshape(coord_add_p, newshape=[3 * 3, 1, 1, 2]) #data_size =3\n",
    "    coord_add_p = np.tile(coord_add_p, [batch_size_pred, D, num_classes, 1])\n",
    "    r_conv_caps1_p = np.ones([5*5*batch_size_pred, 72, C]) / C\n",
    "    r_conv_caps2_p = np.ones([3*3*batch_size_pred, 144, D]) / D\n",
    "    r_class_caps_p = np.ones([3*3*batch_size_pred, 16, num_classes]) / num_classes\n",
    "    \n",
    "    # --------- initialization for training -------------------------------------------\n",
    "    batch_size_train = batch_size\n",
    "    coord_add_t = get_coord_add('mnist') \n",
    "    coord_add_t = np.reshape(coord_add_t, newshape=[3 * 3, 1, 1, 2]) #data_size =3\n",
    "    coord_add_t = np.tile(coord_add_t, [batch_size_train, D, num_classes, 1])\n",
    "    r_conv_caps1_t = np.ones([5*5*batch_size_train, 72, C]) / C\n",
    "    r_conv_caps2_t = np.ones([3*3*batch_size_train, 144, D]) / D\n",
    "    r_class_caps_t = np.ones([3*3*batch_size_train, 16, num_classes]) / num_classes\n",
    "    #------------------------------------ ENDS ---------------------------------------\n",
    "    \n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    episode = 0\n",
    "    while True:\n",
    "        # choose an action epsilon greedily\n",
    "        # readout_t = readout.eval(feed_dict = {s : [s_t].reshape((1,80,80,4))})[0]\n",
    "        readout_t = readout.eval(feed_dict = {s:s_t.reshape((1,80,80,4)),\n",
    "                                              r_conv_caps1: r_conv_caps1_p,\n",
    "                                              r_conv_caps2: r_conv_caps2_p,\n",
    "                                              r_class_caps: r_class_caps_p,\n",
    "                                              coord_add_op_class_caps: coord_add_p,\n",
    "                                              bs: np.int32(batch_size_pred)})\n",
    "        #readout_t = readout.eval(feed_dict = {s:s_t.reshape((1,84,84,4)), coeff:b_IJ1})\n",
    "        \n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "        if random.random() <= epsilon or t <= OBSERVE:\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else:\n",
    "            action_index = np.argmax(readout_t)\n",
    "            a_t[action_index] = 1\n",
    "\n",
    "        # scale down epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observe next state and reward\n",
    "        x_t1_col, r_t, terminal, bar1_score, bar2_score = game_state.frame_step(a_t)\n",
    "        if(terminal == 1):\n",
    "            episode +=1\n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_col, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, x_t1 = cv2.threshold(x_t1,1,255,cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "        s_t1 = np.append(x_t1, s_t[:,:,0:3], axis = 2)\n",
    "\n",
    "        # store the transition in D\n",
    "        replay_memory.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        if len(replay_memory ) > REPLAY_MEMORY:\n",
    "            replay_memory.popleft()\n",
    "            \n",
    "        # only train if done observing\n",
    "        if t > OBSERVE and t%train_freq==0:\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(replay_memory , BATCH)\n",
    "            \n",
    "            # get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "            y_batch = []\n",
    "            readout_j1_batch = readout.eval(feed_dict = {s:s_j1_batch,\n",
    "                                                         r_conv_caps1: r_conv_caps1_t,\n",
    "                                                         r_conv_caps2: r_conv_caps2_t,\n",
    "                                                         r_class_caps: r_class_caps_t,\n",
    "                                                         coord_add_op_class_caps: coord_add_t,\n",
    "                                                         bs: np.int32(batch_size_train)})\n",
    "            #readout_j1_batch = readout.eval(feed_dict = {s:s_j1_batch, coeff:b_IJ2 })\n",
    "\n",
    "            for i in range(0, len(minibatch)):\n",
    "                # if terminal only equals reward\n",
    "                if minibatch[i][4]:\n",
    "                    y_batch.append(r_batch[i])\n",
    "                else:\n",
    "                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "            # perform gradient step\n",
    "            train_step.run(feed_dict = {\n",
    "                y : y_batch,\n",
    "                a : a_batch,\n",
    "                s : s_j_batch,\n",
    "                r_conv_caps1: r_conv_caps1_t,\n",
    "                r_conv_caps2: r_conv_caps2_t,\n",
    "                r_class_caps: r_class_caps_t,\n",
    "                coord_add_op_class_caps: coord_add_t,\n",
    "                bs: np.int32(batch_size_train)})\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        #if t % 10000 == 0:\n",
    "        #    saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
    "        if r_t!= 0:\n",
    "            print (\"TIMESTEP\", t, \"/ e\", episode, \"/ bar1_score\", bar1_score, \"/ bar2_score\", bar2_score, \"/ REWARD\", r_t, \"/ Q_MAX %e\" % np.max(readout_t))\n",
    "\n",
    "        if( (bar1_score - bar2_score) > 18): \n",
    "            print(\"Game_Ends_in Time:\",int(time.time() - tick))\n",
    "            break;   \n",
    "            \n",
    "        # write info to files\n",
    "        '''\n",
    "        if t % 10000 <= 100:\n",
    "            a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
    "            h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
    "            cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame():\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    s, readout, bs, r_conv_caps1, r_conv_caps2, r_class_caps, coord_add_op_class_caps = createNetwork()\n",
    "    trainNetwork(sess, s, readout, bs, r_conv_caps1, r_conv_caps2, r_class_caps, coord_add_op_class_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-91962e308f30>:35: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Network has been created successfully\n",
      "TIMESTEP 129 / e 0 / bar1_score 0 / bar2_score 1 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 175 / e 0 / bar1_score 0 / bar2_score 2 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 221 / e 0 / bar1_score 0 / bar2_score 3 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 267 / e 0 / bar1_score 0 / bar2_score 4 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 313 / e 0 / bar1_score 0 / bar2_score 5 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 359 / e 0 / bar1_score 0 / bar2_score 6 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 405 / e 0 / bar1_score 0 / bar2_score 7 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 451 / e 0 / bar1_score 0 / bar2_score 8 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 497 / e 0 / bar1_score 0 / bar2_score 9 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 543 / e 0 / bar1_score 0 / bar2_score 10 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 589 / e 0 / bar1_score 0 / bar2_score 11 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 635 / e 0 / bar1_score 0 / bar2_score 12 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 681 / e 0 / bar1_score 0 / bar2_score 13 / REWARD -1 / Q_MAX 0.000000e+00\n",
      "TIMESTEP 810 / e 0 / bar1_score 1 / bar2_score 13 / REWARD 1 / Q_MAX -4.429366e-04\n",
      "TIMESTEP 939 / e 0 / bar1_score 1 / bar2_score 14 / REWARD -1 / Q_MAX -9.458855e-04\n",
      "TIMESTEP 985 / e 0 / bar1_score 1 / bar2_score 15 / REWARD -1 / Q_MAX -1.032681e-03\n",
      "TIMESTEP 1031 / e 0 / bar1_score 1 / bar2_score 16 / REWARD -1 / Q_MAX -1.077702e-03\n",
      "TIMESTEP 1077 / e 0 / bar1_score 1 / bar2_score 17 / REWARD -1 / Q_MAX -1.114313e-03\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    playGame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tick = time.time()\n",
    "    main()\n",
    "    print(\"Game_Ends_in Time:\",int(time.time() - tick))\n",
    "    print(\"____________ END HERE _____________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 12, 12, 32)\n",
      "12\n",
      "(?, 12, 12, 8, 16)\n",
      "(?, 12, 12, 8, 1)\n",
      "(?, 12, 12, 136)\n",
      "(?, 5, 5, 136, 9) fdsggs\n",
      "5\n",
      "1 (?, 72, 17)\n",
      "output shape --------------- (?, 72, 17)\n",
      "activation shape---------------------- (?, 72, 1)\n",
      "w (1, 72, 16, 4, 4)\n",
      "w (?, 72, 16, 4, 4)\n",
      "(?, 72, 16, 16) votes shape\n",
      "Tensor(\"conv_caps1/v/tile___3/Reshape:0\", shape=(?, 72, 16, 16), dtype=float32)  = votes\n",
      "(?, 72, 16) r shape__________\n",
      "(?, 72, 16, 1) r1\n",
      "(?, 72, 16) ap\n",
      "(?, 72, 16, 1) r1\n",
      "(?, 16, 1) r_sum\n",
      "(?, 16, 16) cost_h\n",
      "activation (?, 16)\n",
      "3 (?, 5, 5, 16, 16)\n",
      "activation (?, 5, 5, 16, 1)\n",
      "272\n",
      "5 (?, 5, 5, 272)\n",
      "(?, 3, 3, 272, 9) fdsggs\n",
      "canv_caps2 (?, 144, 17) 3\n",
      "canv_caps2_activation (?, 144, 1) 3\n",
      "w (1, 144, 16, 4, 4)\n",
      "w (?, 144, 16, 4, 4)\n",
      "(?, 144, 16, 16) votes shape\n",
      "144 for 1\n",
      "Tensor(\"conv_caps2/v/tile___3/Reshape:0\", shape=(?, 144, 16, 16), dtype=float32)  = votes\n",
      "(?, 144, 16) r shape__________\n",
      "(?, 144, 16, 1) r1\n",
      "(?, 144, 16) ap\n",
      "(?, 144, 16, 1) r1\n",
      "(?, 16, 1) r_sum\n",
      "(?, 16, 16) cost_h\n",
      "4 (?, 16, 16)\n",
      "4 ---activation (?, 16, 1)\n",
      "w (1, 16, 10, 4, 4)\n",
      "w (?, 16, 10, 4, 4)\n",
      "(?, 16, 10, 16) votes.getshape\n",
      "(?, 16, 10, 18) coorr vote shape after  jnbfv\n",
      "_____ 16\n",
      "Tensor(\"class_caps/v/concat:0\", shape=(?, 16, 10, 18), dtype=float32)  = votes\n",
      "(?, 16, 10) r shape__________\n",
      "(?, 16, 10, 1) r1\n",
      "(?, 16, 10) ap\n",
      "(?, 16, 10, 1) r1\n",
      "(?, 10, 1) r_sum\n",
      "(?, 10, 18) cost_h\n",
      "d op (?, 3, 3, 10)\n",
      "miu  2 (?, 1, 10, 18)\n",
      "output_size_posssss (?, 1, 1, 180)\n",
      "output_size (?, 10, 18)\n",
      "pose_out (?, 10, 18)\n",
      "vector_j (?, 180)\n",
      "output_size (?, 6)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "s, readout, bs, r_conv_caps1, r_conv_caps2, r_class_caps, coord_add_op_class_caps = createNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 12, 12, 32)\n",
      "12\n",
      "(?, 12, 12, 8, 16)\n",
      "(?, 12, 12, 8, 1)\n",
      "(?, 12, 12, 136)\n",
      "(?, 5, 5, 136, 9) fdsggs\n",
      "5\n",
      "1 (?, 72, 17)\n",
      "output shape --------------- (?, 72, 17)\n",
      "activation shape---------------------- (?, 72, 1)\n",
      "w (1, 72, 16, 4, 4)\n",
      "w (?, 72, 16, 4, 4)\n",
      "(?, 72, 16, 16) votes shape\n",
      "Tensor(\"conv_caps1/v/tile___3/Reshape:0\", shape=(?, 72, 16, 16), dtype=float32)  = votes\n",
      "(?, 72, 16) r shape__________\n",
      "(?, 72, 16, 1) r1\n",
      "(?, 72, 16) ap\n",
      "(?, 72, 16, 1) r1\n",
      "(?, 16, 1) r_sum\n",
      "(?, 16, 16) cost_h\n",
      "activation (?, 16)\n",
      "3 (?, 5, 5, 16, 16)\n",
      "activation (?, 5, 5, 16, 1)\n",
      "272\n",
      "5 (?, 5, 5, 272)\n",
      "(?, 3, 3, 272, 9) fdsggs\n",
      "canv_caps2 (?, 144, 17) 3\n",
      "canv_caps2_activation (?, 144, 1) 3\n",
      "w (1, 144, 16, 4, 4)\n",
      "w (?, 144, 16, 4, 4)\n",
      "(?, 144, 16, 16) votes shape\n",
      "144 for 1\n",
      "Tensor(\"conv_caps2/v/tile___3/Reshape:0\", shape=(?, 144, 16, 16), dtype=float32)  = votes\n",
      "(?, 144, 16) r shape__________\n",
      "(?, 144, 16, 1) r1\n",
      "(?, 144, 16) ap\n",
      "(?, 144, 16, 1) r1\n",
      "(?, 16, 1) r_sum\n",
      "(?, 16, 16) cost_h\n",
      "4 (?, 16, 16)\n",
      "4 ---activation (?, 16, 1)\n",
      "w (1, 16, 10, 4, 4)\n",
      "w (?, 16, 10, 4, 4)\n",
      "(?, 16, 10, 16) votes.getshape\n",
      "(?, 16, 10, 18) coorr vote shape after  jnbfv\n",
      "_____ 16\n",
      "Tensor(\"class_caps/v/concat:0\", shape=(?, 16, 10, 18), dtype=float32)  = votes\n",
      "(?, 16, 10) r shape__________\n",
      "(?, 16, 10, 1) r1\n",
      "(?, 16, 10) ap\n",
      "(?, 16, 10, 1) r1\n",
      "(?, 10, 1) r_sum\n",
      "(?, 10, 18) cost_h\n",
      "d op (?, 3, 3, 10)\n",
      "miu  2 (?, 1, 10, 18)\n",
      "output_size_posssss (?, 1, 1, 180)\n",
      "output_size (?, 10, 18)\n",
      "pose_out (?, 10, 18)\n",
      "vector_j (?, 180)\n",
      "output_size (?, 6)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#batch_size = 50\n",
    "X = tf.placeholder(tf.float32, shape=(None, 80, 80, 1), name='X')\n",
    "#X = tf.placeholder(tf.float32, shape=(batch_size, 80, 80, 4), name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, 10], name='Y')\n",
    "#Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y')\n",
    "# -------------------------------------------------------------------------------------\n",
    "bs = tf.placeholder(tf.int32, shape=(), name='bs')\n",
    "#bs = batch_size\n",
    "r_conv_caps1= tf.placeholder(tf.float32,[None, 72, C], name='r_conv_caps1') # 5*5*batch_size\n",
    "#r_conv_caps1= tf.placeholder(tf.float32,[5*5*batch_size, 72, C], name='r_conv_caps1') # 5*5*batch_size\n",
    "\n",
    "r_conv_caps2 = tf.placeholder(tf.float32,[None, 144, D], name='r_conv_caps2') # 3*3*batch_size\n",
    "#r_conv_caps2 = tf.placeholder(tf.float32,[3*3*batch_size, 144, D], name='r_conv_caps2') # 3*3*batch_size\n",
    "\n",
    "r_class_caps = tf.placeholder(tf.float32,[None, 16, num_classes], name='r_class_caps') # 3*3*batch_size\n",
    "#r_class_caps = tf.placeholder(tf.float32,[3*3*batch_size, 16, num_classes], name='r_class_caps') # 3*3*batch_size\n",
    "\n",
    "#coord_add_op_class_caps  = tf.placeholder(tf.float32,[3*3*batch_size, 16, num_classes, 2], name='coord_add_op_class_caps')\n",
    "                                                                 # 3*3*batch_size\n",
    "coord_add_op_class_caps  = tf.placeholder(tf.float32,[None, 16, num_classes, 2], name='coord_add_op_class_caps')\n",
    "# 3*3*batch_size\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "data_size = int(X.get_shape()[1])\n",
    "# xavier initialization is necessary here to provide higher stability\n",
    "initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "# instead of initializing bias with constant 0, \n",
    "# a truncated normal initializer is exploited here for higher stability \n",
    "bias_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)  # tf.constant_initializer(0.0)\n",
    "# The paper didnot mention any regularization, a common l2 regularizer to weights is added here\n",
    "weights_regularizer = tf.contrib.layers.l2_regularizer(5e-04)\n",
    "# weights_initializer=initializer,\n",
    "with slim.arg_scope([slim.conv2d], trainable=is_train, biases_initializer=bias_initializer, weights_regularizer=weights_regularizer):\n",
    "    with tf.variable_scope('relu_conv1'):\n",
    "        output = slim.conv2d(X, num_outputs=A, kernel_size=[10, 10], stride=6, padding='VALID', scope='relu_conv1', activation_fn=tf.nn.relu)\n",
    "        data_size = int(np.floor((data_size - 10) / 6)) + 1\n",
    "        print(output.get_shape())\n",
    "        print(data_size)\n",
    "        #assert output.get_shape() == [batch_size, data_size, data_size, 32]\n",
    "        votes__1 = output\n",
    "    with tf.variable_scope('primary_caps'):\n",
    "        pose = slim.conv2d(output, num_outputs=B * 16,kernel_size=[1, 1], stride=1, padding='VALID', scope='primary_caps', activation_fn=None)\n",
    "        activation = slim.conv2d(output, num_outputs=B, kernel_size=[\n",
    "                                 1, 1], stride=1, padding='VALID', scope='primary_caps/activation', activation_fn=tf.nn.sigmoid)\n",
    "        pose = tf.reshape(pose, shape=[-1, data_size, data_size, B, 16]) # (50, 12, 12, 8, 16)\n",
    "        \n",
    "        print(pose.get_shape())\n",
    "        activation = tf.reshape(activation, shape=[-1, data_size, data_size, B, 1]) # (50, 12, 12, 8, 1)\n",
    "        print(activation.get_shape())\n",
    "        output = tf.concat([pose, activation], axis=4)\n",
    "        output = tf.reshape(output, shape=[-1, data_size, data_size, B * 17]) # (50, 12, 12, 136)\n",
    "        print(output.get_shape())\n",
    "        \n",
    "        #assert output.get_shape() == [batch_size, data_size, data_size, B * 17]\n",
    "    with tf.variable_scope('conv_caps1') as scope:\n",
    "        output = kernel_tile(output, 3, 2)\n",
    "        data_size = int(np.floor((data_size - 2) / 2))\n",
    "        print(data_size) # 5 \n",
    "        output = tf.reshape(output, shape=[-1, 3 * 3 * B, 17]) \n",
    "        # batch_size * data_size * data_size  (1250, 72, 17) \n",
    "        print(\"1\",output.get_shape())\n",
    "        activation = tf.reshape(output[:, :, 16], shape=[-1, 3 * 3 * B, 1])\n",
    "        print(\"output shape ---------------\",output.get_shape())\n",
    "        print(\"activation shape----------------------\",activation.get_shape()) #  (1250, 72, 1)\n",
    "        \n",
    "        with tf.variable_scope('v') as scope:\n",
    "            votes = mat_transform(output[:, :, :16], C, weights_regularizer, bs = bs*data_size*data_size)\n",
    "            #bs*data_size*data_size)\n",
    "            \n",
    "            print(votes.get_shape(),\"votes shape\")\n",
    "        with tf.variable_scope('routing') as scope:\n",
    "            caps_num_i = int(activation.get_shape()[1])\n",
    "            \n",
    "            miu, activation, _ = em_routing(votes, activation, C, weights_regularizer,r_conv_caps1)\n",
    "            # miu, activation, _ = em_routing(votes, activation, C, weights_regularizer)\n",
    "            print(\"activation\",activation.get_shape())\n",
    "        pose = tf.reshape(miu, shape=[-1, data_size, data_size, C, 16])\n",
    "        print(\"3\",pose.get_shape()) # 50, 5, 5, 16, 16)\n",
    "        activation = tf.reshape(activation, shape=[-1, data_size, data_size, C, 1])\n",
    "        print(\"activation\",activation.get_shape())\n",
    "        cat_size =  activation.get_shape()[3]*activation.get_shape()[4] + pose.get_shape()[3] *pose.get_shape()[4]\n",
    "        print(cat_size)\n",
    "        output = tf.reshape(tf.concat([pose, activation], axis=4),[-1, data_size, data_size, cat_size])\n",
    "        print(\"5\",output.get_shape()) # (50, 5, 5, 272)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('conv_caps2') as scope:\n",
    "        output = kernel_tile(output, 3, 1)\n",
    "        \n",
    "        data_size = int(np.floor((data_size - 2) / 1))\n",
    "        \n",
    "        output = tf.reshape(output, shape=[-1, 3 * 3 * C, 17]) # batch_size * data_size * data_size\n",
    "        print(\"canv_caps2\",output.get_shape(), data_size)\n",
    "        activation = tf.reshape(output[:, :, 16], shape=[-1 , 3 * 3 * C, 1]) # batch_size * data_size * data_size\n",
    "        print(\"canv_caps2_activation\",activation.get_shape(), data_size)\n",
    "        \n",
    "        with tf.variable_scope('v') as scope:\n",
    "            votes = mat_transform(output[:, :, :16], D, weights_regularizer,bs = bs*data_size*data_size)\n",
    "            print(votes.get_shape(),\"votes shape\")\n",
    "            \n",
    "        with tf.variable_scope('routing') as scope:\n",
    "            caps_num_i = int(activation.get_shape()[1])\n",
    "            print(caps_num_i,\"for 1\")\n",
    "            miu, activation, _ = em_routing(votes, activation, D, weights_regularizer, r_conv_caps2)\n",
    "\n",
    "        pose = tf.reshape(miu, shape=[-1, D, 16]) # batch_size * data_size * data_size\n",
    "        print(\"4\",pose.get_shape())\n",
    "        #tf.logging.info('conv cap 2 pose shape: {}'.format(votes.get_shape()))\n",
    "        activation = tf.reshape(activation, shape=[-1, D, 1]) # batch_size * data_size * data_size\n",
    "        print(\"4 ---activation\",activation.get_shape())\n",
    "        \n",
    "    with tf.variable_scope('class_caps') as scope:\n",
    "        with tf.variable_scope('v') as scope:\n",
    "            votes = mat_transform(pose, num_classes, weights_regularizer,bs = bs*data_size*data_size)\n",
    "            print(votes.get_shape(),\"votes.getshape\")\n",
    "            assert votes.get_shape()[1:] == [D, num_classes, 16]\n",
    "            #tf.logging.info('class cap votes original shape: {}'.format(votes.get_shape()))\n",
    "            '''coord_add = get_coord_add('mnist') \n",
    "            coord_add = np.reshape(coord_add, newshape=[data_size * data_size, 1, 1, 2])\n",
    "            coord_add = np.tile(coord_add, [bs, D, num_classes, 1])\n",
    "            coord_add_op = tf.constant(coord_add, dtype=tf.float32)\n",
    "            print(\"___coord_add______\",coord_add_op.shape)'''\n",
    "            \n",
    "            votes = tf.concat([coord_add_op_class_caps, votes], axis=3)\n",
    "            #tf.logging.info('class cap votes coord add shape: {}'.format(votes.get_shape()))\n",
    "            print(votes.get_shape(),\"coorr vote shape after  jnbfv\")\n",
    "        with tf.variable_scope('routing') as scope:\n",
    "            caps_num_i = int(activation.get_shape()[1])\n",
    "            print(\"_____\",caps_num_i)\n",
    "            miu, activation, test2 = em_routing(votes, activation, num_classes, weights_regularizer,r_class_caps)\n",
    "            \n",
    "        output = tf.reshape(activation, shape=[-1, data_size, data_size, num_classes]) #batch_size\n",
    "        print(\"d op\",output.get_shape())\n",
    "    output = tf.reshape(tf.nn.avg_pool(output, ksize=[1, data_size, data_size, 1], strides=[\n",
    "                1, 1, 1, 1], padding='VALID'), shape=[-1, num_classes]) # batch_size\n",
    "    print(\"miu  2\",miu.get_shape())\n",
    "    pose = tf.nn.avg_pool(tf.reshape(miu, shape=[-1, data_size, data_size,miu.get_shape()[2]*miu.get_shape()[3]\n",
    "                                ]), ksize=[1, data_size, data_size, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    print(\"output_size_posssss\",pose.get_shape())\n",
    "    #miu  2 (450, 1, 10, 18)\n",
    "    #output_size_posssss (50, 1, 1, 180)\n",
    "    pose_out = tf.reshape(pose, shape=[-1, num_classes, 18])\n",
    "    print(\"output_size\",pose_out.get_shape())\n",
    "    vector_j = tf.reshape(pose_out, shape= [-1, num_classes * 18])\n",
    "    print(\"pose_out\",pose_out.get_shape())\n",
    "    print(\"vector_j\",vector_j.get_shape())\n",
    "with tf.variable_scope('output_layer') as scope:\n",
    "    logits = tf.contrib.layers.fully_connected(vector_j, num_outputs=ACTIONS, activation_fn=None)\n",
    "print(\"output_size\",logits.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-62-3a57a218cc8c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-3a57a218cc8c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    final shape pose (50, 12, 12, 128)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(50, 12, 12, 32)\n",
    "final shape pose (50, 12, 12, 128)\n",
    "final shape activation (50, 12, 12, 8)\n",
    "(50, 12, 12, 8, 16)\n",
    "(50, 12, 12, 8, 1)\n",
    "(50, 12, 12, 136)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
